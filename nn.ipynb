{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Neural Network\n",
    "\n",
    "A neural network uses the pre-trained ResNet50 as fixed features extractor. The last convolutional output of the pre-trained models, the bottleneck features, is fed as input to our model. The number of training, validation and test tensors is 176,131, 29,355, 29,356, respectively.\n",
    "\n",
    "\n",
    "## 1. Load Tensors and Targets\n",
    "\n",
    "The training, validation and test tensors along with the associated targets are loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common import *\n",
    "\n",
    "bottleneck_features = np.load('data/bottleneck_features/yelp_resnet50_all.npz')\n",
    "train_resnet50 = bottleneck_features['train_features']\n",
    "valid_resnet50 = bottleneck_features['valid_features']\n",
    "test_resnet50 = bottleneck_features['test_features']\n",
    "\n",
    "train_targets = bottleneck_features['train_targets']\n",
    "valid_targets = bottleneck_features['valid_targets']\n",
    "test_targets = bottleneck_features['test_targets']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_1 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 9)                 2313      \n",
      "=================================================================\n",
      "Total params: 2,763,785.0\n",
      "Trainable params: 2,760,201.0\n",
      "Non-trainable params: 3,584.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import GlobalAveragePooling2D, MaxPooling2D \n",
    "from keras.layers import Conv2D, Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "\n",
    "resnet50 = Sequential()\n",
    "resnet50.add(GlobalAveragePooling2D(input_shape=train_resnet50.shape[1:]))\n",
    "\n",
    "resnet50.add(Dense(1024, activation='relu', kernel_regularizer=l2(1e-4)))\n",
    "resnet50.add(BatchNormalization())\n",
    "resnet50.add(Dropout(0.3))\n",
    "\n",
    "resnet50.add(Dense(512, activation='relu', kernel_regularizer=l2(1e-4)))\n",
    "resnet50.add(BatchNormalization())\n",
    "resnet50.add(Dropout(0.3))\n",
    "\n",
    "resnet50.add(Dense(256, activation='relu', kernel_regularizer=l2(1e-4)))\n",
    "resnet50.add(BatchNormalization())\n",
    "\n",
    "resnet50.add(Dense(9, activation='sigmoid'))\n",
    "\n",
    "resnet50.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compilation and Minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "sgd = SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "resnet50.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 176131 samples, validate on 29355 samples\n",
      "Epoch 1/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.9976 - acc: 0.5819 - val_loss: 0.8558 - val_acc: 0.6634\n",
      "Epoch 2/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.9069 - acc: 0.6450 - val_loss: 0.8130 - val_acc: 0.6995\n",
      "Epoch 3/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.8654 - acc: 0.6730 - val_loss: 0.7897 - val_acc: 0.7176\n",
      "Epoch 4/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.8382 - acc: 0.6899 - val_loss: 0.7741 - val_acc: 0.7279\n",
      "Epoch 5/1000\n",
      "176131/176131 [==============================] - 28s 159us/step - loss: 0.8181 - acc: 0.7024 - val_loss: 0.7619 - val_acc: 0.7360\n",
      "Epoch 6/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.8031 - acc: 0.7110 - val_loss: 0.7527 - val_acc: 0.7415\n",
      "Epoch 7/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.7916 - acc: 0.7179 - val_loss: 0.7457 - val_acc: 0.7447\n",
      "Epoch 8/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.7819 - acc: 0.7233 - val_loss: 0.7404 - val_acc: 0.7475\n",
      "Epoch 9/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.7738 - acc: 0.7273 - val_loss: 0.7356 - val_acc: 0.7495\n",
      "Epoch 10/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.7680 - acc: 0.7302 - val_loss: 0.7319 - val_acc: 0.7514\n",
      "Epoch 11/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.7628 - acc: 0.7326 - val_loss: 0.7286 - val_acc: 0.7533\n",
      "Epoch 12/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.7575 - acc: 0.7358 - val_loss: 0.7255 - val_acc: 0.7547\n",
      "Epoch 13/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.7536 - acc: 0.7374 - val_loss: 0.7229 - val_acc: 0.7561\n",
      "Epoch 14/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.7497 - acc: 0.7392 - val_loss: 0.7207 - val_acc: 0.7572\n",
      "Epoch 15/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.7465 - acc: 0.7411 - val_loss: 0.7186 - val_acc: 0.7583\n",
      "Epoch 16/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.7429 - acc: 0.7430 - val_loss: 0.7167 - val_acc: 0.7590\n",
      "Epoch 17/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.7405 - acc: 0.7439 - val_loss: 0.7150 - val_acc: 0.7600\n",
      "Epoch 18/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.7378 - acc: 0.7455 - val_loss: 0.7133 - val_acc: 0.7608\n",
      "Epoch 19/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.7361 - acc: 0.7461 - val_loss: 0.7120 - val_acc: 0.7613\n",
      "Epoch 20/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.7343 - acc: 0.7472 - val_loss: 0.7106 - val_acc: 0.7620\n",
      "Epoch 21/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.7320 - acc: 0.7483 - val_loss: 0.7092 - val_acc: 0.7626\n",
      "Epoch 22/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.7300 - acc: 0.7492 - val_loss: 0.7079 - val_acc: 0.7634\n",
      "Epoch 23/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.7289 - acc: 0.7497 - val_loss: 0.7067 - val_acc: 0.7640\n",
      "Epoch 24/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.7271 - acc: 0.7504 - val_loss: 0.7055 - val_acc: 0.7645\n",
      "Epoch 25/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.7253 - acc: 0.7516 - val_loss: 0.7046 - val_acc: 0.7650\n",
      "Epoch 26/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.7241 - acc: 0.7519 - val_loss: 0.7036 - val_acc: 0.7654\n",
      "Epoch 27/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.7223 - acc: 0.7531 - val_loss: 0.7028 - val_acc: 0.7659\n",
      "Epoch 28/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.7210 - acc: 0.7534 - val_loss: 0.7017 - val_acc: 0.7661\n",
      "Epoch 29/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.7199 - acc: 0.7541 - val_loss: 0.7007 - val_acc: 0.7666\n",
      "Epoch 30/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.7181 - acc: 0.7552 - val_loss: 0.6999 - val_acc: 0.7669\n",
      "Epoch 31/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.7171 - acc: 0.7553 - val_loss: 0.6990 - val_acc: 0.7675\n",
      "Epoch 32/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.7162 - acc: 0.7560 - val_loss: 0.6982 - val_acc: 0.7679\n",
      "Epoch 33/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.7143 - acc: 0.7568 - val_loss: 0.6974 - val_acc: 0.7683\n",
      "Epoch 34/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.7136 - acc: 0.7573 - val_loss: 0.6966 - val_acc: 0.7686\n",
      "Epoch 35/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.7131 - acc: 0.7575 - val_loss: 0.6959 - val_acc: 0.7689\n",
      "Epoch 36/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.7114 - acc: 0.7585 - val_loss: 0.6950 - val_acc: 0.7694\n",
      "Epoch 37/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.7106 - acc: 0.7587 - val_loss: 0.6942 - val_acc: 0.7695\n",
      "Epoch 38/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.7093 - acc: 0.7596 - val_loss: 0.6936 - val_acc: 0.7699\n",
      "Epoch 39/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.7083 - acc: 0.7598 - val_loss: 0.6929 - val_acc: 0.7699\n",
      "Epoch 40/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.7078 - acc: 0.7597 - val_loss: 0.6923 - val_acc: 0.7701\n",
      "Epoch 41/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.7066 - acc: 0.7605 - val_loss: 0.6918 - val_acc: 0.7705\n",
      "Epoch 42/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.7058 - acc: 0.7606 - val_loss: 0.6910 - val_acc: 0.7707\n",
      "Epoch 43/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.7050 - acc: 0.7613 - val_loss: 0.6905 - val_acc: 0.7708\n",
      "Epoch 44/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.7043 - acc: 0.7613 - val_loss: 0.6898 - val_acc: 0.7713\n",
      "Epoch 45/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.7036 - acc: 0.7621 - val_loss: 0.6893 - val_acc: 0.7714\n",
      "Epoch 46/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.7022 - acc: 0.7624 - val_loss: 0.6885 - val_acc: 0.7718\n",
      "Epoch 47/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.7020 - acc: 0.7628 - val_loss: 0.6882 - val_acc: 0.7718\n",
      "Epoch 48/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.7010 - acc: 0.7628 - val_loss: 0.6874 - val_acc: 0.7721\n",
      "Epoch 49/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.7003 - acc: 0.7638 - val_loss: 0.6867 - val_acc: 0.7727\n",
      "Epoch 50/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6993 - acc: 0.7639 - val_loss: 0.6862 - val_acc: 0.7727\n",
      "Epoch 51/1000\n",
      "176131/176131 [==============================] - 28s 159us/step - loss: 0.6988 - acc: 0.7643 - val_loss: 0.6856 - val_acc: 0.7728\n",
      "Epoch 52/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.6981 - acc: 0.7646 - val_loss: 0.6852 - val_acc: 0.7730\n",
      "Epoch 53/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6980 - acc: 0.7643 - val_loss: 0.6846 - val_acc: 0.7732\n",
      "Epoch 54/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6965 - acc: 0.7649 - val_loss: 0.6840 - val_acc: 0.7738\n",
      "Epoch 55/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6957 - acc: 0.7657 - val_loss: 0.6836 - val_acc: 0.7741\n",
      "Epoch 56/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6949 - acc: 0.7660 - val_loss: 0.6830 - val_acc: 0.7740\n",
      "Epoch 57/1000\n",
      "176131/176131 [==============================] - 28s 159us/step - loss: 0.6945 - acc: 0.7661 - val_loss: 0.6824 - val_acc: 0.7741\n",
      "Epoch 58/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6940 - acc: 0.7665 - val_loss: 0.6818 - val_acc: 0.7746\n",
      "Epoch 59/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6930 - acc: 0.7668 - val_loss: 0.6814 - val_acc: 0.7746\n",
      "Epoch 60/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6917 - acc: 0.7674 - val_loss: 0.6809 - val_acc: 0.7747\n",
      "Epoch 61/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6921 - acc: 0.7667 - val_loss: 0.6804 - val_acc: 0.7748\n",
      "Epoch 62/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6907 - acc: 0.7677 - val_loss: 0.6799 - val_acc: 0.7751\n",
      "Epoch 63/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6904 - acc: 0.7678 - val_loss: 0.6795 - val_acc: 0.7752\n",
      "Epoch 64/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6896 - acc: 0.7681 - val_loss: 0.6789 - val_acc: 0.7752\n",
      "Epoch 65/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6891 - acc: 0.7683 - val_loss: 0.6785 - val_acc: 0.7753\n",
      "Epoch 66/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6886 - acc: 0.7686 - val_loss: 0.6780 - val_acc: 0.7758\n",
      "Epoch 67/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6883 - acc: 0.7686 - val_loss: 0.6776 - val_acc: 0.7754\n",
      "Epoch 68/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6874 - acc: 0.7692 - val_loss: 0.6771 - val_acc: 0.7758\n",
      "Epoch 69/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6873 - acc: 0.7690 - val_loss: 0.6766 - val_acc: 0.7760\n",
      "Epoch 70/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6865 - acc: 0.7692 - val_loss: 0.6761 - val_acc: 0.7766\n",
      "Epoch 71/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6857 - acc: 0.7697 - val_loss: 0.6757 - val_acc: 0.7764\n",
      "Epoch 72/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6850 - acc: 0.7701 - val_loss: 0.6753 - val_acc: 0.7766\n",
      "Epoch 73/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6847 - acc: 0.7702 - val_loss: 0.6748 - val_acc: 0.7765\n",
      "Epoch 74/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6841 - acc: 0.7702 - val_loss: 0.6744 - val_acc: 0.7767\n",
      "Epoch 75/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6838 - acc: 0.7708 - val_loss: 0.6740 - val_acc: 0.7767\n",
      "Epoch 76/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6825 - acc: 0.7712 - val_loss: 0.6735 - val_acc: 0.7770\n",
      "Epoch 77/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6821 - acc: 0.7712 - val_loss: 0.6731 - val_acc: 0.7770\n",
      "Epoch 78/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6818 - acc: 0.7714 - val_loss: 0.6726 - val_acc: 0.7773\n",
      "Epoch 79/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6814 - acc: 0.7717 - val_loss: 0.6723 - val_acc: 0.7774\n",
      "Epoch 80/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6809 - acc: 0.7715 - val_loss: 0.6719 - val_acc: 0.7774\n",
      "Epoch 81/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6804 - acc: 0.7720 - val_loss: 0.6714 - val_acc: 0.7777\n",
      "Epoch 82/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6793 - acc: 0.7726 - val_loss: 0.6710 - val_acc: 0.7779\n",
      "Epoch 83/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6789 - acc: 0.7726 - val_loss: 0.6706 - val_acc: 0.7779\n",
      "Epoch 84/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6788 - acc: 0.7724 - val_loss: 0.6703 - val_acc: 0.7779\n",
      "Epoch 85/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6778 - acc: 0.7732 - val_loss: 0.6698 - val_acc: 0.7782\n",
      "Epoch 86/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6775 - acc: 0.7729 - val_loss: 0.6696 - val_acc: 0.7782\n",
      "Epoch 87/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6773 - acc: 0.7731 - val_loss: 0.6691 - val_acc: 0.7784\n",
      "Epoch 88/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6764 - acc: 0.7736 - val_loss: 0.6687 - val_acc: 0.7784\n",
      "Epoch 89/1000\n",
      "176131/176131 [==============================] - 28s 159us/step - loss: 0.6757 - acc: 0.7737 - val_loss: 0.6682 - val_acc: 0.7787\n",
      "Epoch 90/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6753 - acc: 0.7738 - val_loss: 0.6679 - val_acc: 0.7787\n",
      "Epoch 91/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6752 - acc: 0.7741 - val_loss: 0.6675 - val_acc: 0.7789\n",
      "Epoch 92/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6746 - acc: 0.7743 - val_loss: 0.6671 - val_acc: 0.7791\n",
      "Epoch 93/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6742 - acc: 0.7744 - val_loss: 0.6669 - val_acc: 0.7791\n",
      "Epoch 94/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6735 - acc: 0.7746 - val_loss: 0.6664 - val_acc: 0.7793\n",
      "Epoch 95/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6730 - acc: 0.7750 - val_loss: 0.6660 - val_acc: 0.7795\n",
      "Epoch 96/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6729 - acc: 0.7749 - val_loss: 0.6657 - val_acc: 0.7795\n",
      "Epoch 97/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6717 - acc: 0.7755 - val_loss: 0.6652 - val_acc: 0.7798\n",
      "Epoch 98/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6716 - acc: 0.7755 - val_loss: 0.6649 - val_acc: 0.7797\n",
      "Epoch 99/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6713 - acc: 0.7753 - val_loss: 0.6646 - val_acc: 0.7796\n",
      "Epoch 100/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6700 - acc: 0.7759 - val_loss: 0.6642 - val_acc: 0.7799\n",
      "Epoch 101/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6702 - acc: 0.7758 - val_loss: 0.6639 - val_acc: 0.7801\n",
      "Epoch 102/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6697 - acc: 0.7759 - val_loss: 0.6635 - val_acc: 0.7802\n",
      "Epoch 103/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6689 - acc: 0.7762 - val_loss: 0.6631 - val_acc: 0.7803\n",
      "Epoch 104/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6684 - acc: 0.7767 - val_loss: 0.6628 - val_acc: 0.7803\n",
      "Epoch 105/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6683 - acc: 0.7766 - val_loss: 0.6625 - val_acc: 0.7803\n",
      "Epoch 106/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6673 - acc: 0.7773 - val_loss: 0.6624 - val_acc: 0.7804\n",
      "Epoch 107/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6673 - acc: 0.7773 - val_loss: 0.6618 - val_acc: 0.7806\n",
      "Epoch 108/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6672 - acc: 0.7772 - val_loss: 0.6614 - val_acc: 0.7807\n",
      "Epoch 109/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6662 - acc: 0.7774 - val_loss: 0.6611 - val_acc: 0.7808\n",
      "Epoch 110/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6658 - acc: 0.7775 - val_loss: 0.6607 - val_acc: 0.7807\n",
      "Epoch 111/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6655 - acc: 0.7777 - val_loss: 0.6604 - val_acc: 0.7811\n",
      "Epoch 112/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6651 - acc: 0.7779 - val_loss: 0.6602 - val_acc: 0.7811\n",
      "Epoch 113/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6647 - acc: 0.7780 - val_loss: 0.6598 - val_acc: 0.7814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6638 - acc: 0.7781 - val_loss: 0.6594 - val_acc: 0.7813\n",
      "Epoch 115/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6631 - acc: 0.7788 - val_loss: 0.6592 - val_acc: 0.7813\n",
      "Epoch 116/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6627 - acc: 0.7786 - val_loss: 0.6588 - val_acc: 0.7816\n",
      "Epoch 117/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6624 - acc: 0.7792 - val_loss: 0.6586 - val_acc: 0.7815\n",
      "Epoch 118/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6626 - acc: 0.7788 - val_loss: 0.6583 - val_acc: 0.7816\n",
      "Epoch 119/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6622 - acc: 0.7787 - val_loss: 0.6577 - val_acc: 0.7819\n",
      "Epoch 120/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6616 - acc: 0.7791 - val_loss: 0.6577 - val_acc: 0.7817\n",
      "Epoch 121/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6608 - acc: 0.7795 - val_loss: 0.6573 - val_acc: 0.7820\n",
      "Epoch 122/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6608 - acc: 0.7795 - val_loss: 0.6569 - val_acc: 0.7819\n",
      "Epoch 123/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6601 - acc: 0.7799 - val_loss: 0.6565 - val_acc: 0.7822\n",
      "Epoch 124/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6598 - acc: 0.7796 - val_loss: 0.6563 - val_acc: 0.7822\n",
      "Epoch 125/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6591 - acc: 0.7801 - val_loss: 0.6560 - val_acc: 0.7823\n",
      "Epoch 126/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6591 - acc: 0.7800 - val_loss: 0.6558 - val_acc: 0.7820\n",
      "Epoch 127/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6584 - acc: 0.7803 - val_loss: 0.6554 - val_acc: 0.7823\n",
      "Epoch 128/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6581 - acc: 0.7806 - val_loss: 0.6550 - val_acc: 0.7825\n",
      "Epoch 129/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6573 - acc: 0.7807 - val_loss: 0.6548 - val_acc: 0.7824\n",
      "Epoch 130/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6575 - acc: 0.7807 - val_loss: 0.6543 - val_acc: 0.7827\n",
      "Epoch 131/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6564 - acc: 0.7813 - val_loss: 0.6541 - val_acc: 0.7826\n",
      "Epoch 132/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6566 - acc: 0.7809 - val_loss: 0.6539 - val_acc: 0.7827\n",
      "Epoch 133/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6558 - acc: 0.7814 - val_loss: 0.6537 - val_acc: 0.7827\n",
      "Epoch 134/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6550 - acc: 0.7816 - val_loss: 0.6535 - val_acc: 0.7827\n",
      "Epoch 135/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6551 - acc: 0.7815 - val_loss: 0.6530 - val_acc: 0.7830\n",
      "Epoch 136/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6543 - acc: 0.7822 - val_loss: 0.6528 - val_acc: 0.7831\n",
      "Epoch 137/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6541 - acc: 0.7821 - val_loss: 0.6525 - val_acc: 0.7829\n",
      "Epoch 138/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6534 - acc: 0.7824 - val_loss: 0.6522 - val_acc: 0.7832\n",
      "Epoch 139/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6533 - acc: 0.7824 - val_loss: 0.6519 - val_acc: 0.7832\n",
      "Epoch 140/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6537 - acc: 0.7820 - val_loss: 0.6517 - val_acc: 0.7832\n",
      "Epoch 141/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6522 - acc: 0.7831 - val_loss: 0.6513 - val_acc: 0.7832\n",
      "Epoch 142/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6517 - acc: 0.7830 - val_loss: 0.6510 - val_acc: 0.7835\n",
      "Epoch 143/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6519 - acc: 0.7830 - val_loss: 0.6508 - val_acc: 0.7833\n",
      "Epoch 144/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6515 - acc: 0.7830 - val_loss: 0.6505 - val_acc: 0.7835\n",
      "Epoch 145/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6507 - acc: 0.7835 - val_loss: 0.6503 - val_acc: 0.7834\n",
      "Epoch 146/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6509 - acc: 0.7830 - val_loss: 0.6501 - val_acc: 0.7836\n",
      "Epoch 147/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.6499 - acc: 0.7835 - val_loss: 0.6497 - val_acc: 0.7838\n",
      "Epoch 148/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6497 - acc: 0.7838 - val_loss: 0.6494 - val_acc: 0.7837\n",
      "Epoch 149/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6494 - acc: 0.7841 - val_loss: 0.6491 - val_acc: 0.7837\n",
      "Epoch 150/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.6492 - acc: 0.7837 - val_loss: 0.6487 - val_acc: 0.7838\n",
      "Epoch 151/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.6488 - acc: 0.7839 - val_loss: 0.6486 - val_acc: 0.7839\n",
      "Epoch 152/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.6478 - acc: 0.7844 - val_loss: 0.6483 - val_acc: 0.7840\n",
      "Epoch 153/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6481 - acc: 0.7841 - val_loss: 0.6481 - val_acc: 0.7839\n",
      "Epoch 154/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6472 - acc: 0.7844 - val_loss: 0.6478 - val_acc: 0.7840\n",
      "Epoch 155/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6473 - acc: 0.7846 - val_loss: 0.6476 - val_acc: 0.7841\n",
      "Epoch 156/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.6466 - acc: 0.7849 - val_loss: 0.6473 - val_acc: 0.7840\n",
      "Epoch 157/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6459 - acc: 0.7853 - val_loss: 0.6470 - val_acc: 0.7840\n",
      "Epoch 158/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6460 - acc: 0.7849 - val_loss: 0.6468 - val_acc: 0.7842\n",
      "Epoch 159/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6454 - acc: 0.7849 - val_loss: 0.6465 - val_acc: 0.7843\n",
      "Epoch 160/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6451 - acc: 0.7852 - val_loss: 0.6464 - val_acc: 0.7841\n",
      "Epoch 161/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6447 - acc: 0.7853 - val_loss: 0.6461 - val_acc: 0.7844\n",
      "Epoch 162/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6443 - acc: 0.7853 - val_loss: 0.6458 - val_acc: 0.7846\n",
      "Epoch 163/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.6437 - acc: 0.7863 - val_loss: 0.6457 - val_acc: 0.7844\n",
      "Epoch 164/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6436 - acc: 0.7860 - val_loss: 0.6453 - val_acc: 0.7847\n",
      "Epoch 165/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.6428 - acc: 0.7862 - val_loss: 0.6452 - val_acc: 0.7844\n",
      "Epoch 166/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6428 - acc: 0.7862 - val_loss: 0.6448 - val_acc: 0.7848\n",
      "Epoch 167/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6423 - acc: 0.7863 - val_loss: 0.6446 - val_acc: 0.7847\n",
      "Epoch 168/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.6420 - acc: 0.7866 - val_loss: 0.6444 - val_acc: 0.7848\n",
      "Epoch 169/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.6414 - acc: 0.7870 - val_loss: 0.6442 - val_acc: 0.7848\n",
      "Epoch 170/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6409 - acc: 0.7869 - val_loss: 0.6439 - val_acc: 0.7849\n",
      "Epoch 171/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6407 - acc: 0.7871 - val_loss: 0.6437 - val_acc: 0.7849\n",
      "Epoch 172/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6405 - acc: 0.7870 - val_loss: 0.6435 - val_acc: 0.7849\n",
      "Epoch 173/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.6400 - acc: 0.7872 - val_loss: 0.6433 - val_acc: 0.7849\n",
      "Epoch 174/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6399 - acc: 0.7874 - val_loss: 0.6430 - val_acc: 0.7850\n",
      "Epoch 175/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6397 - acc: 0.7873 - val_loss: 0.6427 - val_acc: 0.7851\n",
      "Epoch 176/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.6393 - acc: 0.7873 - val_loss: 0.6425 - val_acc: 0.7850\n",
      "Epoch 177/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.6384 - acc: 0.7874 - val_loss: 0.6424 - val_acc: 0.7851\n",
      "Epoch 178/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.6384 - acc: 0.7878 - val_loss: 0.6420 - val_acc: 0.7851\n",
      "Epoch 179/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.6383 - acc: 0.7874 - val_loss: 0.6417 - val_acc: 0.7853\n",
      "Epoch 180/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6376 - acc: 0.7879 - val_loss: 0.6416 - val_acc: 0.7853\n",
      "Epoch 181/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6371 - acc: 0.7883 - val_loss: 0.6414 - val_acc: 0.7853\n",
      "Epoch 182/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.6369 - acc: 0.7884 - val_loss: 0.6411 - val_acc: 0.7855\n",
      "Epoch 183/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.6364 - acc: 0.7886 - val_loss: 0.6409 - val_acc: 0.7854\n",
      "Epoch 184/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.6356 - acc: 0.7888 - val_loss: 0.6408 - val_acc: 0.7855\n",
      "Epoch 185/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.6353 - acc: 0.7887 - val_loss: 0.6406 - val_acc: 0.7856\n",
      "Epoch 186/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.6356 - acc: 0.7887 - val_loss: 0.6403 - val_acc: 0.7855\n",
      "Epoch 187/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6346 - acc: 0.7894 - val_loss: 0.6400 - val_acc: 0.7857\n",
      "Epoch 188/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6346 - acc: 0.7892 - val_loss: 0.6398 - val_acc: 0.7856\n",
      "Epoch 189/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6342 - acc: 0.7893 - val_loss: 0.6398 - val_acc: 0.7855\n",
      "Epoch 190/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6342 - acc: 0.7892 - val_loss: 0.6395 - val_acc: 0.7858\n",
      "Epoch 191/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.6336 - acc: 0.7896 - val_loss: 0.6392 - val_acc: 0.7857\n",
      "Epoch 192/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6331 - acc: 0.7896 - val_loss: 0.6391 - val_acc: 0.7858\n",
      "Epoch 193/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6330 - acc: 0.7894 - val_loss: 0.6388 - val_acc: 0.7858\n",
      "Epoch 194/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6327 - acc: 0.7895 - val_loss: 0.6386 - val_acc: 0.7859\n",
      "Epoch 195/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6327 - acc: 0.7895 - val_loss: 0.6384 - val_acc: 0.7858\n",
      "Epoch 196/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6317 - acc: 0.7902 - val_loss: 0.6382 - val_acc: 0.7857\n",
      "Epoch 197/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.6316 - acc: 0.7899 - val_loss: 0.6380 - val_acc: 0.7859\n",
      "Epoch 198/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6309 - acc: 0.7901 - val_loss: 0.6378 - val_acc: 0.7858\n",
      "Epoch 199/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.6308 - acc: 0.7904 - val_loss: 0.6375 - val_acc: 0.7861\n",
      "Epoch 200/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6301 - acc: 0.7906 - val_loss: 0.6374 - val_acc: 0.7861\n",
      "Epoch 201/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6304 - acc: 0.7904 - val_loss: 0.6373 - val_acc: 0.7861\n",
      "Epoch 202/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6300 - acc: 0.7904 - val_loss: 0.6370 - val_acc: 0.7860\n",
      "Epoch 203/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6293 - acc: 0.7911 - val_loss: 0.6369 - val_acc: 0.7861\n",
      "Epoch 204/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.6292 - acc: 0.7908 - val_loss: 0.6367 - val_acc: 0.7862\n",
      "Epoch 205/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6288 - acc: 0.7909 - val_loss: 0.6362 - val_acc: 0.7865\n",
      "Epoch 206/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6284 - acc: 0.7912 - val_loss: 0.6362 - val_acc: 0.7866\n",
      "Epoch 207/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6280 - acc: 0.7915 - val_loss: 0.6360 - val_acc: 0.7862\n",
      "Epoch 208/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6277 - acc: 0.7916 - val_loss: 0.6359 - val_acc: 0.7865\n",
      "Epoch 209/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6267 - acc: 0.7920 - val_loss: 0.6357 - val_acc: 0.7864\n",
      "Epoch 210/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6269 - acc: 0.7921 - val_loss: 0.6356 - val_acc: 0.7864\n",
      "Epoch 211/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6269 - acc: 0.7919 - val_loss: 0.6352 - val_acc: 0.7866\n",
      "Epoch 212/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6259 - acc: 0.7921 - val_loss: 0.6350 - val_acc: 0.7866\n",
      "Epoch 213/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6255 - acc: 0.7923 - val_loss: 0.6350 - val_acc: 0.7866\n",
      "Epoch 214/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.6251 - acc: 0.7926 - val_loss: 0.6346 - val_acc: 0.7868\n",
      "Epoch 215/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6252 - acc: 0.7925 - val_loss: 0.6345 - val_acc: 0.7869\n",
      "Epoch 216/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6248 - acc: 0.7925 - val_loss: 0.6342 - val_acc: 0.7868\n",
      "Epoch 217/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6243 - acc: 0.7930 - val_loss: 0.6341 - val_acc: 0.7870\n",
      "Epoch 218/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6242 - acc: 0.7928 - val_loss: 0.6339 - val_acc: 0.7871\n",
      "Epoch 219/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6232 - acc: 0.7934 - val_loss: 0.6337 - val_acc: 0.7870\n",
      "Epoch 220/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6235 - acc: 0.7930 - val_loss: 0.6335 - val_acc: 0.7871\n",
      "Epoch 221/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6222 - acc: 0.7937 - val_loss: 0.6333 - val_acc: 0.7872\n",
      "Epoch 222/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6229 - acc: 0.7931 - val_loss: 0.6331 - val_acc: 0.7874\n",
      "Epoch 223/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6226 - acc: 0.7933 - val_loss: 0.6331 - val_acc: 0.7869\n",
      "Epoch 224/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6219 - acc: 0.7938 - val_loss: 0.6329 - val_acc: 0.7873\n",
      "Epoch 225/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6216 - acc: 0.7940 - val_loss: 0.6326 - val_acc: 0.7874\n",
      "Epoch 226/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6216 - acc: 0.7936 - val_loss: 0.6324 - val_acc: 0.7873\n",
      "Epoch 227/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6210 - acc: 0.7938 - val_loss: 0.6324 - val_acc: 0.7874\n",
      "Epoch 228/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6205 - acc: 0.7943 - val_loss: 0.6321 - val_acc: 0.7875\n",
      "Epoch 229/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6200 - acc: 0.7945 - val_loss: 0.6318 - val_acc: 0.7875\n",
      "Epoch 230/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6197 - acc: 0.7947 - val_loss: 0.6318 - val_acc: 0.7873\n",
      "Epoch 231/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6197 - acc: 0.7945 - val_loss: 0.6315 - val_acc: 0.7875\n",
      "Epoch 232/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6194 - acc: 0.7943 - val_loss: 0.6315 - val_acc: 0.7874\n",
      "Epoch 233/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6190 - acc: 0.7949 - val_loss: 0.6312 - val_acc: 0.7875\n",
      "Epoch 234/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6189 - acc: 0.7947 - val_loss: 0.6309 - val_acc: 0.7875\n",
      "Epoch 235/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6190 - acc: 0.7944 - val_loss: 0.6307 - val_acc: 0.7875\n",
      "Epoch 236/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6180 - acc: 0.7953 - val_loss: 0.6308 - val_acc: 0.7875\n",
      "Epoch 237/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6178 - acc: 0.7950 - val_loss: 0.6305 - val_acc: 0.7877\n",
      "Epoch 238/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6173 - acc: 0.7953 - val_loss: 0.6303 - val_acc: 0.7878\n",
      "Epoch 239/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6171 - acc: 0.7954 - val_loss: 0.6302 - val_acc: 0.7877\n",
      "Epoch 240/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6166 - acc: 0.7956 - val_loss: 0.6300 - val_acc: 0.7879\n",
      "Epoch 241/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6163 - acc: 0.7958 - val_loss: 0.6298 - val_acc: 0.7875\n",
      "Epoch 242/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6155 - acc: 0.7962 - val_loss: 0.6298 - val_acc: 0.7877\n",
      "Epoch 243/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6157 - acc: 0.7960 - val_loss: 0.6295 - val_acc: 0.7878\n",
      "Epoch 244/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6153 - acc: 0.7962 - val_loss: 0.6295 - val_acc: 0.7876\n",
      "Epoch 245/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6152 - acc: 0.7963 - val_loss: 0.6290 - val_acc: 0.7879\n",
      "Epoch 246/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6146 - acc: 0.7964 - val_loss: 0.6291 - val_acc: 0.7879\n",
      "Epoch 247/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6138 - acc: 0.7966 - val_loss: 0.6289 - val_acc: 0.7881\n",
      "Epoch 248/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6135 - acc: 0.7970 - val_loss: 0.6290 - val_acc: 0.7880\n",
      "Epoch 249/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6136 - acc: 0.7969 - val_loss: 0.6286 - val_acc: 0.7880\n",
      "Epoch 250/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6134 - acc: 0.7969 - val_loss: 0.6285 - val_acc: 0.7883\n",
      "Epoch 251/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6129 - acc: 0.7968 - val_loss: 0.6285 - val_acc: 0.7880\n",
      "Epoch 252/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6131 - acc: 0.7968 - val_loss: 0.6280 - val_acc: 0.7883\n",
      "Epoch 253/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6117 - acc: 0.7974 - val_loss: 0.6281 - val_acc: 0.7881\n",
      "Epoch 254/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6119 - acc: 0.7975 - val_loss: 0.6278 - val_acc: 0.7882\n",
      "Epoch 255/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6118 - acc: 0.7975 - val_loss: 0.6277 - val_acc: 0.7883\n",
      "Epoch 256/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6117 - acc: 0.7975 - val_loss: 0.6276 - val_acc: 0.7885\n",
      "Epoch 257/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6114 - acc: 0.7974 - val_loss: 0.6274 - val_acc: 0.7881\n",
      "Epoch 258/1000\n",
      "176131/176131 [==============================] - 28s 159us/step - loss: 0.6114 - acc: 0.7976 - val_loss: 0.6273 - val_acc: 0.7882\n",
      "Epoch 259/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6107 - acc: 0.7977 - val_loss: 0.6269 - val_acc: 0.7885\n",
      "Epoch 260/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6099 - acc: 0.7980 - val_loss: 0.6268 - val_acc: 0.7884\n",
      "Epoch 261/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6098 - acc: 0.7978 - val_loss: 0.6268 - val_acc: 0.7884\n",
      "Epoch 262/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6096 - acc: 0.7981 - val_loss: 0.6266 - val_acc: 0.7883\n",
      "Epoch 263/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6099 - acc: 0.7981 - val_loss: 0.6266 - val_acc: 0.7883\n",
      "Epoch 264/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6086 - acc: 0.7983 - val_loss: 0.6262 - val_acc: 0.7883\n",
      "Epoch 265/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6082 - acc: 0.7987 - val_loss: 0.6263 - val_acc: 0.7884\n",
      "Epoch 266/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6085 - acc: 0.7987 - val_loss: 0.6262 - val_acc: 0.7883\n",
      "Epoch 267/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6082 - acc: 0.7989 - val_loss: 0.6260 - val_acc: 0.7885\n",
      "Epoch 268/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6079 - acc: 0.7985 - val_loss: 0.6258 - val_acc: 0.7884\n",
      "Epoch 269/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.6078 - acc: 0.7985 - val_loss: 0.6256 - val_acc: 0.7884\n",
      "Epoch 270/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6066 - acc: 0.7994 - val_loss: 0.6256 - val_acc: 0.7887\n",
      "Epoch 271/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6069 - acc: 0.7990 - val_loss: 0.6252 - val_acc: 0.7887\n",
      "Epoch 272/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6067 - acc: 0.7992 - val_loss: 0.6249 - val_acc: 0.7885\n",
      "Epoch 273/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6059 - acc: 0.7994 - val_loss: 0.6249 - val_acc: 0.7887\n",
      "Epoch 274/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6059 - acc: 0.7995 - val_loss: 0.6248 - val_acc: 0.7886\n",
      "Epoch 275/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6060 - acc: 0.7992 - val_loss: 0.6247 - val_acc: 0.7886\n",
      "Epoch 276/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6053 - acc: 0.7997 - val_loss: 0.6246 - val_acc: 0.7888\n",
      "Epoch 277/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6050 - acc: 0.8000 - val_loss: 0.6245 - val_acc: 0.7886\n",
      "Epoch 278/1000\n",
      "176131/176131 [==============================] - 28s 159us/step - loss: 0.6042 - acc: 0.8002 - val_loss: 0.6241 - val_acc: 0.7889\n",
      "Epoch 279/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6036 - acc: 0.8005 - val_loss: 0.6242 - val_acc: 0.7887\n",
      "Epoch 280/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6041 - acc: 0.8004 - val_loss: 0.6238 - val_acc: 0.7890\n",
      "Epoch 281/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6037 - acc: 0.8000 - val_loss: 0.6237 - val_acc: 0.7890\n",
      "Epoch 282/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6033 - acc: 0.8004 - val_loss: 0.6238 - val_acc: 0.7890\n",
      "Epoch 283/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6030 - acc: 0.8007 - val_loss: 0.6237 - val_acc: 0.7889\n",
      "Epoch 284/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6026 - acc: 0.8006 - val_loss: 0.6234 - val_acc: 0.7889\n",
      "Epoch 285/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6026 - acc: 0.8007 - val_loss: 0.6232 - val_acc: 0.7888\n",
      "Epoch 286/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6013 - acc: 0.8014 - val_loss: 0.6229 - val_acc: 0.7892\n",
      "Epoch 287/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6013 - acc: 0.8014 - val_loss: 0.6231 - val_acc: 0.7890\n",
      "Epoch 288/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6009 - acc: 0.8014 - val_loss: 0.6229 - val_acc: 0.7890\n",
      "Epoch 289/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6008 - acc: 0.8016 - val_loss: 0.6226 - val_acc: 0.7892\n",
      "Epoch 290/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6007 - acc: 0.8014 - val_loss: 0.6227 - val_acc: 0.7891\n",
      "Epoch 291/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.6007 - acc: 0.8014 - val_loss: 0.6225 - val_acc: 0.7892\n",
      "Epoch 292/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.6005 - acc: 0.8014 - val_loss: 0.6224 - val_acc: 0.7893\n",
      "Epoch 293/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.5998 - acc: 0.8018 - val_loss: 0.6224 - val_acc: 0.7893\n",
      "Epoch 294/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.5999 - acc: 0.8016 - val_loss: 0.6221 - val_acc: 0.7893\n",
      "Epoch 295/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.5991 - acc: 0.8023 - val_loss: 0.6219 - val_acc: 0.7891\n",
      "Epoch 296/1000\n",
      "176131/176131 [==============================] - 29s 162us/step - loss: 0.5990 - acc: 0.8021 - val_loss: 0.6220 - val_acc: 0.7893\n",
      "Epoch 297/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5988 - acc: 0.8020 - val_loss: 0.6218 - val_acc: 0.7894\n",
      "Epoch 298/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.5985 - acc: 0.8023 - val_loss: 0.6216 - val_acc: 0.7893\n",
      "Epoch 299/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.5984 - acc: 0.8024 - val_loss: 0.6215 - val_acc: 0.7894\n",
      "Epoch 300/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.5975 - acc: 0.8025 - val_loss: 0.6215 - val_acc: 0.7893\n",
      "Epoch 301/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.5973 - acc: 0.8023 - val_loss: 0.6214 - val_acc: 0.7893\n",
      "Epoch 302/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.5964 - acc: 0.8032 - val_loss: 0.6211 - val_acc: 0.7896\n",
      "Epoch 303/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.5971 - acc: 0.8028 - val_loss: 0.6210 - val_acc: 0.7897\n",
      "Epoch 304/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.5965 - acc: 0.8029 - val_loss: 0.6209 - val_acc: 0.7895\n",
      "Epoch 305/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.5962 - acc: 0.8031 - val_loss: 0.6205 - val_acc: 0.7896\n",
      "Epoch 306/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.5960 - acc: 0.8030 - val_loss: 0.6204 - val_acc: 0.7896\n",
      "Epoch 307/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.5955 - acc: 0.8033 - val_loss: 0.6206 - val_acc: 0.7894\n",
      "Epoch 308/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.5950 - acc: 0.8034 - val_loss: 0.6203 - val_acc: 0.7895\n",
      "Epoch 309/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.5946 - acc: 0.8036 - val_loss: 0.6202 - val_acc: 0.7895\n",
      "Epoch 310/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.5945 - acc: 0.8037 - val_loss: 0.6201 - val_acc: 0.7897\n",
      "Epoch 311/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.5946 - acc: 0.8038 - val_loss: 0.6201 - val_acc: 0.7895\n",
      "Epoch 312/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.5942 - acc: 0.8035 - val_loss: 0.6199 - val_acc: 0.7897\n",
      "Epoch 313/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.5939 - acc: 0.8040 - val_loss: 0.6197 - val_acc: 0.7895\n",
      "Epoch 314/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.5929 - acc: 0.8042 - val_loss: 0.6198 - val_acc: 0.7896\n",
      "Epoch 315/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.5934 - acc: 0.8039 - val_loss: 0.6197 - val_acc: 0.7897\n",
      "Epoch 316/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.5927 - acc: 0.8042 - val_loss: 0.6199 - val_acc: 0.7896\n",
      "Epoch 317/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.5924 - acc: 0.8047 - val_loss: 0.6193 - val_acc: 0.7897\n",
      "Epoch 318/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.5925 - acc: 0.8042 - val_loss: 0.6191 - val_acc: 0.7898\n",
      "Epoch 319/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.5924 - acc: 0.8047 - val_loss: 0.6191 - val_acc: 0.7898\n",
      "Epoch 320/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.5920 - acc: 0.8050 - val_loss: 0.6192 - val_acc: 0.7898\n",
      "Epoch 321/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.5915 - acc: 0.8050 - val_loss: 0.6190 - val_acc: 0.7898\n",
      "Epoch 322/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.5906 - acc: 0.8054 - val_loss: 0.6187 - val_acc: 0.7898\n",
      "Epoch 323/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.5905 - acc: 0.8052 - val_loss: 0.6187 - val_acc: 0.7899\n",
      "Epoch 324/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.5905 - acc: 0.8051 - val_loss: 0.6184 - val_acc: 0.7899\n",
      "Epoch 325/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.5906 - acc: 0.8052 - val_loss: 0.6187 - val_acc: 0.7898\n",
      "Epoch 326/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.5897 - acc: 0.8056 - val_loss: 0.6183 - val_acc: 0.7898\n",
      "Epoch 327/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.5901 - acc: 0.8054 - val_loss: 0.6183 - val_acc: 0.7898\n",
      "Epoch 328/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.5891 - acc: 0.8057 - val_loss: 0.6181 - val_acc: 0.7897\n",
      "Epoch 329/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.5894 - acc: 0.8053 - val_loss: 0.6184 - val_acc: 0.7899\n",
      "Epoch 330/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.5888 - acc: 0.8059 - val_loss: 0.6179 - val_acc: 0.7900\n",
      "Epoch 331/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.5886 - acc: 0.8060 - val_loss: 0.6177 - val_acc: 0.7899\n",
      "Epoch 332/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.5885 - acc: 0.8060 - val_loss: 0.6177 - val_acc: 0.7899\n",
      "Epoch 333/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.5877 - acc: 0.8063 - val_loss: 0.6176 - val_acc: 0.7899\n",
      "Epoch 334/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.5876 - acc: 0.8062 - val_loss: 0.6172 - val_acc: 0.7901\n",
      "Epoch 335/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.5880 - acc: 0.8059 - val_loss: 0.6169 - val_acc: 0.7900\n",
      "Epoch 336/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.5872 - acc: 0.8063 - val_loss: 0.6172 - val_acc: 0.7902\n",
      "Epoch 337/1000\n",
      "176131/176131 [==============================] - 28s 157us/step - loss: 0.5865 - acc: 0.8066 - val_loss: 0.6171 - val_acc: 0.7900\n",
      "Epoch 338/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.5865 - acc: 0.8062 - val_loss: 0.6170 - val_acc: 0.7901\n",
      "Epoch 339/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.5859 - acc: 0.8072 - val_loss: 0.6169 - val_acc: 0.7902\n",
      "Epoch 340/1000\n",
      "176131/176131 [==============================] - 27s 156us/step - loss: 0.5863 - acc: 0.8065 - val_loss: 0.6169 - val_acc: 0.7902\n",
      "Epoch 341/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.5856 - acc: 0.8071 - val_loss: 0.6171 - val_acc: 0.7900\n",
      "Epoch 342/1000\n",
      "176131/176131 [==============================] - 28s 156us/step - loss: 0.5857 - acc: 0.8069 - val_loss: 0.6167 - val_acc: 0.7900\n",
      "Epoch 343/1000\n",
      "176131/176131 [==============================] - 28s 158us/step - loss: 0.5850 - acc: 0.8075 - val_loss: 0.6164 - val_acc: 0.7901\n",
      "Epoch 344/1000\n",
      "176131/176131 [==============================] - 28s 161us/step - loss: 0.5849 - acc: 0.8073 - val_loss: 0.6162 - val_acc: 0.7902\n",
      "Epoch 345/1000\n",
      "176131/176131 [==============================] - 28s 159us/step - loss: 0.5845 - acc: 0.8076 - val_loss: 0.6164 - val_acc: 0.7899\n",
      "Epoch 346/1000\n",
      "176131/176131 [==============================] - 28s 159us/step - loss: 0.5845 - acc: 0.8071 - val_loss: 0.6160 - val_acc: 0.7903\n",
      "Epoch 347/1000\n",
      "176131/176131 [==============================] - 28s 159us/step - loss: 0.5839 - acc: 0.8077 - val_loss: 0.6164 - val_acc: 0.7903\n",
      "Epoch 348/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5837 - acc: 0.8077 - val_loss: 0.6159 - val_acc: 0.7903\n",
      "Epoch 349/1000\n",
      "176131/176131 [==============================] - 28s 161us/step - loss: 0.5834 - acc: 0.8077 - val_loss: 0.6158 - val_acc: 0.7903\n",
      "Epoch 350/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5832 - acc: 0.8077 - val_loss: 0.6158 - val_acc: 0.7903\n",
      "Epoch 351/1000\n",
      "176131/176131 [==============================] - 28s 159us/step - loss: 0.5825 - acc: 0.8081 - val_loss: 0.6157 - val_acc: 0.7902\n",
      "Epoch 352/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5824 - acc: 0.8080 - val_loss: 0.6154 - val_acc: 0.7902\n",
      "Epoch 353/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5823 - acc: 0.8077 - val_loss: 0.6152 - val_acc: 0.7905\n",
      "Epoch 354/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5820 - acc: 0.8085 - val_loss: 0.6158 - val_acc: 0.7902\n",
      "Epoch 355/1000\n",
      "176131/176131 [==============================] - 28s 161us/step - loss: 0.5823 - acc: 0.8078 - val_loss: 0.6154 - val_acc: 0.7903\n",
      "Epoch 356/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5819 - acc: 0.8081 - val_loss: 0.6147 - val_acc: 0.7905\n",
      "Epoch 357/1000\n",
      "176131/176131 [==============================] - 28s 159us/step - loss: 0.5819 - acc: 0.8080 - val_loss: 0.6152 - val_acc: 0.7904\n",
      "Epoch 358/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5809 - acc: 0.8089 - val_loss: 0.6147 - val_acc: 0.7905\n",
      "Epoch 359/1000\n",
      "176131/176131 [==============================] - 28s 159us/step - loss: 0.5806 - acc: 0.8086 - val_loss: 0.6147 - val_acc: 0.7905\n",
      "Epoch 360/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5808 - acc: 0.8085 - val_loss: 0.6145 - val_acc: 0.7904\n",
      "Epoch 361/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5798 - acc: 0.8089 - val_loss: 0.6146 - val_acc: 0.7904\n",
      "Epoch 362/1000\n",
      "176131/176131 [==============================] - 28s 161us/step - loss: 0.5802 - acc: 0.8086 - val_loss: 0.6149 - val_acc: 0.7904\n",
      "Epoch 363/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5799 - acc: 0.8090 - val_loss: 0.6144 - val_acc: 0.7905\n",
      "Epoch 364/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5798 - acc: 0.8088 - val_loss: 0.6142 - val_acc: 0.7908\n",
      "Epoch 365/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5789 - acc: 0.8095 - val_loss: 0.6141 - val_acc: 0.7905\n",
      "Epoch 366/1000\n",
      "176131/176131 [==============================] - 28s 159us/step - loss: 0.5783 - acc: 0.8096 - val_loss: 0.6141 - val_acc: 0.7906\n",
      "Epoch 367/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5786 - acc: 0.8095 - val_loss: 0.6140 - val_acc: 0.7905\n",
      "Epoch 368/1000\n",
      "176131/176131 [==============================] - 28s 161us/step - loss: 0.5787 - acc: 0.8096 - val_loss: 0.6137 - val_acc: 0.7907\n",
      "Epoch 369/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5783 - acc: 0.8093 - val_loss: 0.6137 - val_acc: 0.7907\n",
      "Epoch 370/1000\n",
      "176131/176131 [==============================] - 28s 159us/step - loss: 0.5784 - acc: 0.8093 - val_loss: 0.6139 - val_acc: 0.7904\n",
      "Epoch 371/1000\n",
      "176131/176131 [==============================] - 28s 159us/step - loss: 0.5774 - acc: 0.8098 - val_loss: 0.6138 - val_acc: 0.7908\n",
      "Epoch 372/1000\n",
      "176131/176131 [==============================] - 28s 159us/step - loss: 0.5776 - acc: 0.8096 - val_loss: 0.6136 - val_acc: 0.7907\n",
      "Epoch 373/1000\n",
      "176131/176131 [==============================] - 28s 159us/step - loss: 0.5773 - acc: 0.8099 - val_loss: 0.6133 - val_acc: 0.7908\n",
      "Epoch 374/1000\n",
      "176131/176131 [==============================] - 28s 159us/step - loss: 0.5763 - acc: 0.8102 - val_loss: 0.6134 - val_acc: 0.7908\n",
      "Epoch 375/1000\n",
      "176131/176131 [==============================] - 28s 161us/step - loss: 0.5757 - acc: 0.8106 - val_loss: 0.6130 - val_acc: 0.7907\n",
      "Epoch 376/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5767 - acc: 0.8101 - val_loss: 0.6134 - val_acc: 0.7907\n",
      "Epoch 377/1000\n",
      "176131/176131 [==============================] - 28s 159us/step - loss: 0.5755 - acc: 0.8106 - val_loss: 0.6128 - val_acc: 0.7909\n",
      "Epoch 378/1000\n",
      "176131/176131 [==============================] - 28s 159us/step - loss: 0.5757 - acc: 0.8107 - val_loss: 0.6134 - val_acc: 0.7905\n",
      "Epoch 379/1000\n",
      "176131/176131 [==============================] - 28s 159us/step - loss: 0.5755 - acc: 0.8107 - val_loss: 0.6130 - val_acc: 0.7908\n",
      "Epoch 380/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5754 - acc: 0.8110 - val_loss: 0.6127 - val_acc: 0.7908\n",
      "Epoch 381/1000\n",
      "176131/176131 [==============================] - 28s 161us/step - loss: 0.5750 - acc: 0.8107 - val_loss: 0.6124 - val_acc: 0.7909\n",
      "Epoch 382/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5745 - acc: 0.8111 - val_loss: 0.6125 - val_acc: 0.7911\n",
      "Epoch 383/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5740 - acc: 0.8112 - val_loss: 0.6124 - val_acc: 0.7906\n",
      "Epoch 384/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5739 - acc: 0.8114 - val_loss: 0.6124 - val_acc: 0.7909\n",
      "Epoch 385/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5737 - acc: 0.8111 - val_loss: 0.6127 - val_acc: 0.7907\n",
      "Epoch 386/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5729 - acc: 0.8117 - val_loss: 0.6123 - val_acc: 0.7909\n",
      "Epoch 387/1000\n",
      "176131/176131 [==============================] - 28s 161us/step - loss: 0.5731 - acc: 0.8118 - val_loss: 0.6124 - val_acc: 0.7909\n",
      "Epoch 388/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5719 - acc: 0.8122 - val_loss: 0.6122 - val_acc: 0.7909\n",
      "Epoch 389/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5723 - acc: 0.8116 - val_loss: 0.6120 - val_acc: 0.7910\n",
      "Epoch 390/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5722 - acc: 0.8118 - val_loss: 0.6120 - val_acc: 0.7908\n",
      "Epoch 391/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5724 - acc: 0.8116 - val_loss: 0.6118 - val_acc: 0.7910\n",
      "Epoch 392/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5721 - acc: 0.8118 - val_loss: 0.6119 - val_acc: 0.7908\n",
      "Epoch 393/1000\n",
      "176131/176131 [==============================] - 28s 159us/step - loss: 0.5721 - acc: 0.8115 - val_loss: 0.6119 - val_acc: 0.7910\n",
      "Epoch 394/1000\n",
      "176131/176131 [==============================] - 29s 162us/step - loss: 0.5706 - acc: 0.8126 - val_loss: 0.6117 - val_acc: 0.7910\n",
      "Epoch 395/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5710 - acc: 0.8124 - val_loss: 0.6119 - val_acc: 0.7909\n",
      "Epoch 396/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5707 - acc: 0.8123 - val_loss: 0.6113 - val_acc: 0.7911\n",
      "Epoch 397/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5708 - acc: 0.8122 - val_loss: 0.6116 - val_acc: 0.7911\n",
      "Epoch 398/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5704 - acc: 0.8124 - val_loss: 0.6115 - val_acc: 0.7910\n",
      "Epoch 399/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5698 - acc: 0.8128 - val_loss: 0.6114 - val_acc: 0.7909\n",
      "Epoch 400/1000\n",
      "176131/176131 [==============================] - 28s 161us/step - loss: 0.5697 - acc: 0.8127 - val_loss: 0.6113 - val_acc: 0.7911\n",
      "Epoch 401/1000\n",
      "176131/176131 [==============================] - 28s 160us/step - loss: 0.5690 - acc: 0.8132 - val_loss: 0.6113 - val_acc: 0.7908\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='data/saved_models/weights_resnet50.hdf5', save_best_only=True)\n",
    "checkimprovement = EarlyStopping(monitor='val_loss', min_delta=0, patience=5)\n",
    "\n",
    "history_resnet50 = resnet50.fit(train_resnet50, train_targets, \n",
    "                                validation_data=(valid_resnet50,valid_targets), \n",
    "                                epochs=1000, batch_size=64, verbose=1,\n",
    "                                callbacks=[checkpointer,checkimprovement])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of the loss function and the accuracy metric across epochs are plotted below for the training (blue) and validation (red) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history(history_resnet50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model, i.e, the one with the lowest value of the loss function calculated on the validation data, is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet50.load_weights('data/saved_models/weights_resnet50.hdf5')\n",
    "resnet50.save('data/saved_models/model_resnet50.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. $F_1$ score\n",
    "\n",
    "Predictions are made on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = resnet50.predict(test_resnet50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions are grouped by business and an averaged prediction is derived for each business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img, _ = load_dataset('data/preprocess/test.npz')\n",
    "group = group_images(img, 'data/train_photo_to_biz_ids.csv')\n",
    "\n",
    "combine = []\n",
    "label = []\n",
    "for g in group:\n",
    "    combine.append(np.mean(predictions[g],axis=0))\n",
    "    label.append(test_targets[g[0]])\n",
    "\n",
    "combine = np.array(combine)\n",
    "label = np.array(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the $F_1$ score using a unique threshold of 0.5 for all 9 labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.82807\n"
     ]
    }
   ],
   "source": [
    "yhat_unique = (combine >= 0.5).astype(int)\n",
    "score_unique = f1_score(label, yhat_unique)\n",
    "print('F1 score: %.5f' % score_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the $F_1$ score is calculated using a custom list of threshold, i.e., one per label. The thresholds have been derived in this [notebook](findBestThreshold.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.83160\n"
     ]
    }
   ],
   "source": [
    "threshold = np.array([0.43, 0.55, 0.535, 0.525, 0.545, 0.54, 0.55, 0.47, 0.5])\n",
    "yhat_custom = [[1 if combine[i,j] >= threshold[j] else 0 for j in range(9)] for i in range(len(label))]\n",
    "yhat_custom = np.array(yhat_custom)\n",
    "\n",
    "score_custom = f1_score(label, yhat_custom)\n",
    "print('F1 score: %.5f' % score_custom)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
